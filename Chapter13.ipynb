{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjcn4q0MsAU7hWJ/HXFw00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hank199599/data_science_from_scratch_reading_log/blob/main/Chapter13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v56obdLeyaht"
      },
      "source": [
        "# 單純貝氏(Naive Bayes)\n",
        "描述在已知一些條件下，某事件的發生機率。  \n",
        "  \n",
        "**建立前提**：每個事件特徵基本上是獨立的，與其他事件沒有關係。  \n",
        "**特色**：不易發生過擬合\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPlZEX3vaBMA"
      },
      "source": [
        "# 理論：建構垃圾郵件篩選器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNMHdRjZcSWE"
      },
      "source": [
        "## 極為簡單的版本 \n",
        "利用貝氏定理計算「此郵件為垃圾郵件」的機率：\n",
        "  \n",
        "在「此郵件包含bitcoin這個字眼」的事件下，此郵件為垃圾郵件的機率是：\n",
        "```\n",
        "P(S|B)=[P(B|S)P(S)/[P(B|S)P(S)+P(B|¬S)]P(¬S)\n",
        "```\n",
        "* S : 此郵件為垃圾郵件的事件  \n",
        "* B : 此郵件包含bitcoin這個字眼的事件\n",
        "  \n",
        "\n",
        "若手邊有大量郵件確定是垃圾郵件，亦有大量郵件確定不是垃圾郵件。  \n",
        "可以輕易估算出P(B|S)以及P(B|¬S)的值。  \n",
        "  \n",
        "若進一步假設，一封郵件是不是垃圾郵件的機率各占一半。(即P(S)=P(¬S)=0.5)  \n",
        "則原式可以被改寫為：\n",
        "\n",
        "```\n",
        "P(S|B)=[P(B|S)/[P(B|S)P(S)+P(B|¬S)]\n",
        "```\n",
        "假設：\n",
        "* 50%的垃圾郵件含有bitcoin這個字眼\n",
        "* 1%的非垃圾郵件含有bitcoin這個字眼\n",
        "→確實是垃圾郵件的機率是：\n",
        "\n",
        "\n",
        "```\n",
        "0.5/(0.5+0.01)=98%\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA1xsUv-cUxc"
      },
      "source": [
        "## 較為精巧的版本\n",
        "**假設**：  \n",
        "* 有個包含許多單詞w₁,...,wɴ的詞彙表(vocabulary)。  \n",
        "* 以Xi代表「郵件中包含wi這個單詞的事件」\n",
        "  \n",
        "**再假設我們能估計出**：\n",
        "* 垃圾郵件中包含wi的機率P(Xi|S)\n",
        "* 垃圾郵件中不包含wi的機率P(Xi|¬S)\n",
        "  \n",
        "```\n",
        "P(X₁=X₁,...,Xn=xn|S) = P(X₁=x₁|S)×...×P(Xn=xn|S)\n",
        "```\n",
        "\n",
        "假定詞彙表中只有「bitcion」與「rolex」兩個單詞，而在已知所有的垃圾郵件中：  \n",
        "* 一半擁有「earn bitcoin」\n",
        "* 一半擁有「authenic rolex」  \n",
        "\n",
        "在這個情況下，垃圾郵件同時包含「bitcion」與「rolex」兩字眼的機率為0。  \n",
        "若使用單純貝氏估算其機率：\n",
        "```\n",
        "P(X₁=1,X₂=1|S) = P(X₁=1|S)×P(X₂=1|S) = .5×.5 = .25\n",
        "```\n",
        "在這個假設裡，並未考慮到實際上「bitcion」與「rolex」這兩個單詞還是會互相影響。  \n",
        "而非理想上的獨立事件。在忽略理想情形與現實情形有所差距的情況下，這個模型仍經常被運用在現實世界的垃圾郵件篩選器之中。\n",
        "  \n",
        "在實務上，通常會盡量避免很多機率值相乘。  \n",
        "多個小於零的浮點數相乘後，易造成「**下溢(underflow)**」。  \n",
        "根據代數關係：\n",
        "* log(ab)=log(a)+log(b)\n",
        "* exp(log(x))=x  \n",
        "  \n",
        "我們能將原式轉換為：\n",
        "```\n",
        "exp(log(p1)+...+log(pn))\n",
        "```\n",
        "假定我們手邊有大量以標示過的垃圾郵件與非垃圾郵件用於訓練。  \n",
        "(log(x))=x  \n",
        "  \n",
        "我們能將原式轉換為：\n",
        "```\n",
        "exp(log(p1)+...+log(pn))\n",
        "```\n",
        "假定我們手邊有大量以標示過的垃圾郵件與非垃圾郵件用於訓練。  \n",
        "則我們可以估算出**垃圾郵件內容中包含wi這個單詞的機率P(Xi|S)**\n",
        "  \n",
        "然而，在機率計算中仍會出現問題：  \n",
        "假設在我們用於訓練的所有郵件中，詞彙表裡的「data」只出現在非垃圾郵件中。  \n",
        "```\n",
        "P(\"data\"|S)=0\n",
        "```\n",
        "這代表這個分類器只要遇到任何含有「**data**」單詞的郵件會判定為**非垃圾郵件**。\n",
        "即便出現「data on free bitcoin and authetic watches」這類的句子。  \n",
        "  \n",
        "在這種情況下，會使用一個**偽計數值k**。  \n",
        "在估算垃圾郵件中出現wi這個單詞時，採取以下作法：\n",
        "```\n",
        "P(Xi|S) = (k+所有垃圾郵件中含有wi這個單詞的郵件數量)/(2k+所有垃圾郵件的數量)\n",
        "```\n",
        "```\n",
        "P(Xi|¬S) = (k+所有非垃圾郵件中含有wi這個單詞的郵件數量)/(2k+所有非垃圾郵件的數量)\n",
        "```\n",
        "舉例來說：  \n",
        "如果「data」這個單詞在98封垃圾郵件中完全沒出現，而k值設定為1時。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7HvSK5yBaW"
      },
      "source": [
        "# 實作\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blGFZ8tyCJes"
      },
      "source": [
        "## **建立物件的方式**\n",
        "1. 把郵件內容依據單詞進行拆分(tokenize)\n",
        "2. 把各不相同的單詞token集合起來\n",
        "3. 利用re.fill把所有單詞token都提取出來\n",
        "4. 用**set()**整併重複的token\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyI25qhGPfT"
      },
      "source": [
        "from typing import Set\n",
        "import re\n",
        "\n",
        "def tokenize(text:str) ->Set[str]:\n",
        "  text = text.lower()           #轉為小寫\n",
        "  all_words = re.findall(\"[a-z0-9]+\",text) #取出其中的單詞\n",
        "  return set(all_words)           #去掉重複的單詞"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r3Dj-dGDW7i"
      },
      "source": [
        "assert tokenize(\"Data Science is science\") == {\"data\",\"science\",\"is\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqh--EuDua8"
      },
      "source": [
        "## 針對訓練資料定義資料型別 (基本物件)\n",
        "由於分類器須持續記錄訓練資料中出現的單詞token以及各種計數與標籤，因此採用物件類別的方法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl2XJdZ9DuzD"
      },
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class Message(NamedTuple):\n",
        "  text:str\n",
        "  is_spam:bool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McdcbkpCEHur"
      },
      "source": [
        "## 定義處理該資料型別的方法(**method**)\n",
        "\n",
        "* ham ：非垃圾郵件\n",
        "* spam ：垃圾郵件\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from typing import List,Tuple,Dict,Iterable\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "  def __init__(self,k:float=0.5)->None:\n",
        "    self.k = k #平滑因子\n",
        "    \n",
        "    self.tokens:Set[str] = set()\n",
        "    self.token_spam_counts:Dict[str,int] = defaultdict(int)\n",
        "    self.token_ham_counts:Dict[str,int] = defaultdict(int)\n",
        "    self.spam_messages = self.ham_messages = 0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPJOUVz6Mxa_"
      },
      "source": [
        "### 定義**方法**：針對一堆郵件進行訓練\n",
        "**方法**：針對一堆郵件進行訓練\n",
        "1. 累計垃圾郵件與非垃圾郵件數量\n",
        "2. 針對每封郵件進行分詞(tokenize)後的每個單詞token，根據該郵件是否是垃圾郵件來斷定該累計 **token_spam_counts** 還是 **token_ham_counts**\n",
        "\n",
        "\n",
        "```python\n",
        "def train(self,messages:Iterable[Message]) ->None:\n",
        "  for message in messages:\n",
        "    #累計郵件數量\n",
        "    if message.is_spam:\n",
        "      self.spam_messages += 1\n",
        "    else:\n",
        "      self.ham_messages += 1\n",
        "    \n",
        "    #累計單詞出現的次數\n",
        "    for token in tokenize(message.text):\n",
        "      self.tokens.add(token)\n",
        "      if message.is_spam:\n",
        "        self.token_spam_counts[token] += 1\n",
        "      else:\n",
        "        self.token_ham_counts[token] += 1\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhCdnkwPYhy"
      },
      "source": [
        "###  定義**方法**：預測P(*spam*|*token*)此事件的機率值\n",
        "即出現某token的機率下，郵件為垃圾郵件的機率。  \n",
        "  \n",
        "**目標**；得到詞彙表中每個token相應的P(token|spam)與P(token|ham)  \n",
        "**方法**：建立一個private輔助函式來進行計算\n",
        "\n",
        "\n",
        "```python\n",
        "def _probabilities(self,token:str) ->Tuple[float,float]:\n",
        "  \"\"\"送回P(token|spam)與P(token|ham) \"\"\"\n",
        "  spam = self.token_spam_counts[token]\n",
        "  ham = self.token_ham_counts[token]\n",
        "\n",
        "  p_token_spam = (spam + self.k)/(self.spam_messages + 2 * self.k)\n",
        "  p_token_ham = (ham + self.k)/(self.ham_messages + 2 * self.k)\n",
        "\n",
        "  return p_token_spam,p_token_ham\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7dGGmWpRM5b"
      },
      "source": [
        "### 定義**方法**：編寫預測方式\n",
        "在計算最終機率值上，採用對數相加而非直接相乘。\n",
        "\n",
        "```python\n",
        "def predict(self,text:str) ->float:\n",
        "  text_tokens = tokenize(text)\n",
        "  log_prob_if_spam = log_prob_if_ham = 0.0\n",
        "\n",
        "  # 以迭代方式，處理詞彙表中的每個單詞\n",
        "  for token in self.tokens:\n",
        "    prob_if_spam, prob_if_ham = self._probabilities(token)\n",
        "\n",
        "    # 如果*token*有出現在郵件中，\n",
        "    # 就把「有看到該token」的對數機率值加進去\n",
        "    if token in text_tokens:\n",
        "      log_prob_if_spam += math.log(prob_if_spam)\n",
        "      log_prob_if_ham += math.log(prob_if_ham)\n",
        "    \n",
        "    #否則就把「沒有看到該token」的對數機率值加進去\n",
        "    #「沒有看到該token」的對數機率值就是log( 1 - 有看到該token機率值)\n",
        "    else:\n",
        "      log_prob_if_spam += math.log(1.0 - log_prob_if_spam)\n",
        "      log_prob_if_ham += math.log(1.0 - log_prob_if_ham)\n",
        "  \n",
        "  prob_if_spam = math.exp(log_prob_if_spam)\n",
        "  prob_if_ham = math.exp(log_prob_if_ham)\n",
        "  return prob_if_spam/(prob_if_spam + prob_if_ham)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djJ1bViATAd_"
      },
      "source": [
        "### 到此，分類器已建置完成!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vck29wIXWSDo"
      },
      "source": [
        "from typing import List,Tuple,Dict,Iterable\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "  def __init__(self,k:float=0.5)->None:\n",
        "    \"\"\"初始化欲包含的資料結構\"\"\"\n",
        "\n",
        "    self.k = k #平滑因子\n",
        "    self.tokens:Set[str] = set()\n",
        "    self.token_spam_counts:Dict[str,int] = defaultdict(int)\n",
        "    self.token_ham_counts:Dict[str,int] = defaultdict(int)\n",
        "    self.spam_messages = self.ham_messages = 0\n",
        "\n",
        "  def train(self,messages:Iterable[Message]) ->None:\n",
        "    for message in messages:\n",
        "      #累計郵件數量\n",
        "      if message.is_spam:\n",
        "        self.spam_messages += 1\n",
        "      else:\n",
        "        self.ham_messages += 1\n",
        "      \n",
        "      #累計單詞出現的次數\n",
        "      for token in tokenize(message.text):\n",
        "        self.tokens.add(token)\n",
        "        if message.is_spam:\n",
        "          self.token_spam_counts[token] += 1\n",
        "        else:\n",
        "          self.token_ham_counts[token] += 1\n",
        "  \n",
        "  def _probabilities(self,token:str) ->Tuple[float,float]:\n",
        "    \"\"\"預測P(spam|token)此事件的機率值\n",
        "      送回P(token|spam)與P(token|ham) \"\"\"\n",
        "    spam = self.token_spam_counts[token]\n",
        "    ham = self.token_ham_counts[token]\n",
        "\n",
        "    p_token_spam = (spam + self.k)/(self.spam_messages + 2 * self.k)\n",
        "    p_token_ham = (ham + self.k)/(self.ham_messages + 2 * self.k)\n",
        "\n",
        "    return p_token_spam,p_token_ham\n",
        "  \n",
        "  def predict(self,text:str) ->float:\n",
        "    \"\"\"編寫預測方法\"\"\"\n",
        "\n",
        "    text_tokens = tokenize(text)\n",
        "    log_prob_if_spam = log_prob_if_ham = 0.0\n",
        "\n",
        "    # 以迭代方式，處理詞彙表中的每個單詞\n",
        "    for token in self.tokens:\n",
        "      prob_if_spam, prob_if_ham = self._probabilities(token)\n",
        "\n",
        "      # 如果*token*有出現在郵件中，\n",
        "      # 就把「有看到該token」的對數機率值加進去\n",
        "      if token in text_tokens:\n",
        "        log_prob_if_spam += math.log(prob_if_spam)\n",
        "        log_prob_if_ham += math.log(prob_if_ham)\n",
        "      \n",
        "      #否則就把「沒有看到該token」的對數機率值加進去\n",
        "      #「沒有看到該token」的對數機率值就是log( 1 - 有看到該token機率值)\n",
        "      else:\n",
        "        log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "        log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
        "    \n",
        "    prob_if_spam = math.exp(log_prob_if_spam)\n",
        "    prob_if_ham = math.exp(log_prob_if_ham)\n",
        "\n",
        "    return prob_if_spam/(prob_if_spam + prob_if_ham)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOD--8uhyFCG"
      },
      "source": [
        "# 測試模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxE7pSe6zttH"
      },
      "source": [
        "#編寫一些單元測試方法，以確保模型可以正確運作\n",
        "message=[Message(\"spam rules\",is_spam=True),\n",
        "     Message(\"ham rules\",is_spam=False),\n",
        "     Message(\"hello ham\",is_spam=False)]\n",
        "\n",
        "model = NaiveBayesClassifier(k=0.5) #假定平滑因子為0.5\n",
        "model.train(message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUdaE2cqZoH7"
      },
      "source": [
        "###  1.檢查它是否可以正確計算出各種計數值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWG_50hiZ1Lx"
      },
      "source": [
        "assert model.tokens == {\"spam\",\"ham\",\"rules\",\"hello\"}\n",
        "assert model.spam_messages == 1\n",
        "assert model.ham_messages == 2\n",
        "assert model.token_spam_counts =={\"spam\":1,\"rules\":1}\n",
        "assert model.token_ham_counts == {\"ham\":2,\"rules\":1,\"hello\":1}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K71iaNDSZvmH"
      },
      "source": [
        "### 2.進行預測\n",
        "透過手工方式進行單純貝氏邏輯的相關計算，並確認可得到相同的結果：\n",
        "  \n",
        "假定文字是：\"hello spam\"\n",
        "\n",
        "* 分析這段文字：\n",
        "  * 出現\"sapm\"次數：1\n",
        "  * 出現\"ham\"次數：0\n",
        "  * 出現\"rules\"次數：0\n",
        "  * 出現\"hello\"次數：1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHMJyGEazaU"
      },
      "source": [
        "text = \"hello spam\"\n",
        "\n",
        "probs_if_spam =[\n",
        "  (1+0.5)/(1+2*0.5),  #\"spam\"：有這個詞\n",
        "  1-(0+0.5)/(1+2*0.5), #\"ham\"：沒有這個詞\n",
        "  1-(1+0.5)/(1+2*0.5), #\"rules\"：沒有這個詞 \n",
        "  (0+0.5)/(1+2*0.5)   #\"hello\"：有這個詞\n",
        "]\n",
        "\n",
        "probs_if_ham =[\n",
        "  (0+0.5)/(2+2*0.5),  #\"spam\"：有這個詞\n",
        "  1-(2+0.5)/(2+2*0.5), #\"ham\"：沒有這個詞\n",
        "  1-(1+0.5)/(2+2*0.5), #\"rules\"：沒有這個詞 \n",
        "  (1+0.5)/(2+2*0.5)   #\"hello\"：有這個詞\n",
        "]\n",
        "\n",
        "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
        "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emGwrSeD4PJg",
        "outputId": "2f8597a0-549e-4eb9-92a1-095a5fc22999"
      },
      "source": [
        "print('%.3f'%(model.predict(text)))\n",
        "print('%.3f'%(p_if_spam/(p_if_spam+p_if_ham)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.835\n",
            "0.835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mM1CM8iw407"
      },
      "source": [
        "#兩者的值會大略落在0.83附近\n",
        "assert round(model.predict(text),3) == round(p_if_spam/(p_if_spam+p_if_ham),3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hldHr_2fyG7h"
      },
      "source": [
        "# 運用模型\n",
        "採用 [SpamAssassin公共語錄資料集](https://spamassassin.apache.org/old/publiccorpus/) 於2010年的一些檔案來訓練模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDLBQbn1HI6e"
      },
      "source": [
        "### 下載檔案並解壓縮到指定的目錄中\n",
        " \n",
        "書本上採行Python內建的IO函式庫，下載至本地端解壓縮並進行整理。  \n",
        "於CodeLab上實行時，會超出Colab的讀寫上限而被中斷。  \n",
        "改為下載資料集到本地端進行整理後，再透過pandas函式庫對資料進行處理。  \n",
        "以下是經處理後，提取**信件主旨字串**後的檔案備份。  \n",
        "[Github檔案連結](https://github.com/hank199599/data_science_from_scratch_reading_log/tree/main/attached_data/CH%2013)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2T94rnZzuEJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "spam=list(pd.read_csv(\"https://raw.githubusercontent.com/hank199599/data_science_from_scratch_reading_log/main/chapter13%20mail%20subjects/spam.txt\",sep='\\t',header=None)[0]) \n",
        "easy_ham=list(pd.read_csv(\"https://raw.githubusercontent.com/hank199599/data_science_from_scratch_reading_log/main/chapter13%20mail%20subjects/easy_ham.txt\",sep='\\t',header=None)[0]) \n",
        "hard_ham=list(pd.read_csv(\"https://raw.githubusercontent.com/hank199599/data_science_from_scratch_reading_log/main/chapter13%20mail%20subjects/hard_ham.txt\",sep='\\t',header=None)[0]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtWpCkxZ-n1K"
      },
      "source": [
        "對資料進行處理，去除一些沒有用的字元：如果使用the errors='ignore'這個設定。  \n",
        "遇到問題時會直接跳過，而不會丟出例外狀況"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF3dmfb8-6zK"
      },
      "source": [
        "data = []\n",
        "\n",
        "for row in spam:\n",
        "  data.append(Message(row.lstrip('Subject: '),False))\n",
        "\n",
        "for data_set in [easy_ham,hard_ham]:\n",
        "  for row in data_set:\n",
        "    data.append(Message(row.lstrip('Subject: '),True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wjECm8IrB3A"
      },
      "source": [
        "import random\n",
        "from typing import TypeVar,List,Tuple\n",
        "X=TypeVar('X') #以通用型別來代表資料點\n",
        "\n",
        "def split_data(data:List[X],prob:float) ->Tuple[List[X],List[X]]:\n",
        "  \"\"\"把資料依照[prob,1-prob]的比率進行切割\"\"\"\n",
        "  data = data[:]        #複製一份資料\n",
        "  random.shuffle(data)      #因shuffle會打亂資料\n",
        "  cut = int(len(data)*prob)   #用prob算出切分點\n",
        "  return data[:cut],data[cut:]  #用打亂過的資料進行切分"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmx7MhtzrD76"
      },
      "source": [
        "將資料切分為訓練組資料以及測試組資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA6MOhUDrLdO"
      },
      "source": [
        "random.seed(0) #以確保結果與書上一致\n",
        "train_messages , test_messages = split_data(data ,0.75)\n",
        "\n",
        "model = NaiveBayesClassifier()\n",
        "model.train(train_messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhxrKxivxwf"
      },
      "source": [
        "進行一些預測，檢查模型的效果如何："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9ntq6rfv4B9",
        "outputId": "e35c51af-c65d-42b9-bb6d-63597ed5e780"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "predictions = [(message , model.predict(message.text)) for message in test_messages]\n",
        "\n",
        "# 假定spam_probability > 0.5 就表示預測為垃圾郵件\n",
        "# 接著再計算(確實為垃圾郵件，預測為垃圾郵件) 這種組合的數量\n",
        "confusion_matrix = Counter((message.is_spam , spam_probability > 0.5) for message , spam_probability in predictions)\n",
        "\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({(True, True): 456, (False, False): 67, (False, True): 54, (True, False): 12})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsj9sRbix3NW"
      },
      "source": [
        "* 真陽性(True, True): 456\n",
        "* 偽陽性(False, True): 54\n",
        "* 偽陰性(True, False): 12\n",
        "* 真陰性(False, False): 67\n",
        "  \n",
        "**精確率(accuracy)** = 456/(456+54) = 0.89411764705  \n",
        "**召回率(recall)** = 465/(456+12) = 0.99358974359"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uINInCNTzk4S"
      },
      "source": [
        "### 確認模型內部  \n",
        "看看那些單詞容易或最不容易被認為是垃圾郵件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxKzz-eHzxsR"
      },
      "source": [
        "def p_spam_given_token(token:str,model:NaiveBayesClassifier) ->float:\n",
        "  #為何不應該調用Private Model?\n",
        "  prob_if_spam , probs_if_ham = model._probabilities(token)\n",
        "\n",
        "  return prob_if_spam / (prob_if_spam+probs_if_ham)\n",
        "\n",
        "words = sorted(model.tokens , key = lambda t: p_spam_given_token(t,model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwk_1ozP0nB1",
        "outputId": "b90027d3-957a-42c6-976d-0aecc9aa0589"
      },
      "source": [
        "print(\"spammiest_words\",words[-10:])\n",
        "print(\"hammiest_words\",words[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spammiest_words ['apt', 'spamassassin', 'headlines', '09', 'bug', 'perl', 'sadev', 'razor', 'users', 'zzzzteana']\n",
            "hammiest_words ['adv', 'systemworks', 'money', 'sale', 'account', 'clearance', 'zzzz', 'enenkio', 'assistance', 'norton']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96zSVl-71rLu"
      },
      "source": [
        "# 如何獲得更好的效果\n",
        "\n",
        "* 看郵件的內容,而不只是查看郵件的主旨,在處理郵件標頭的部分時一定要特別小心。\n",
        "\n",
        "* 這裡的分類器會把出现在訓練資料中的每個單與全都列入考慮,即使有些單調只出現過一次,我們可以修改分類器,讓它接受一個可有可無的**最低次數(min_count)** 門艦值,然後名略掉出現次數低於這個門艦值的一些單爵。\n",
        "* 在進行分詞時,並沒有考慮到類似單詞(例如*cheap*和*cheapest*)的概念。  \n",
        "我們可以修改分類器,真它接受一個可有可無的 stemmer 函式,這個函式可以把單詞轉成原型單詞。舉例來說,一個個超級簡單的 stemmer 函式可以 成這樣:\n",
        "\n",
        "\n",
        "```python\n",
        "  def drop_final_s(word):\n",
        "    return re.sub(\"s$\", \"\", word)\n",
        "```\n",
        "\n",
        "實際上想要建立一個優秀的 stemmer 函式並不容易。[Porter Summer](https://lartarus.org/martin/PorierStemmer/)則是經常被大家使用的其中一個選擇。\n",
        "\n",
        "* 雖然我們所採用的特徵項都是「內容包含有 wi 這個單詞」這樣的形式,但實際上不見得非如此不可，在我們的實作中,也可以添加一些像是「內容包含某個數字」這樣的額外特徵項,做法上則是可以建立像 **contains:number** 這樣的「偽詞(phony tokens)，然後修改我們的分詞tokenizer函式,讓它能在適當的時候發揮作用。"
      ]
    }
  ]
}