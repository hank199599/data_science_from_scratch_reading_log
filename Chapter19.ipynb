{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter19.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5eLkoE7mfp4rfoX0IYCsj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hank199599/data_science_from_scratch_reading_log/blob/main/Chapter19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-upQJRhBT4RG"
      },
      "source": [
        "# 深度學習(Deep learning)\r\n",
        "原本指的是「深度」神經網路，現被用來泛指各種神經網路。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDNDb28zUSny"
      },
      "source": [
        "# 張量\r\n",
        "在神經網路函示庫中，n維陣列被稱為**張量(tensor)**  \r\n",
        "理想情況下，可以使用：\r\n",
        "\r\n",
        "```python\r\n",
        "# 張量Tensor要不是一個浮點數，就是一個張量列表\r\n",
        "Tensor = Union[float,List[Tensor]]\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woz6w24ma8-S"
      },
      "source": [
        "#如同我們說：\r\n",
        "Tensor = list"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd7FNN-kYo3Q"
      },
      "source": [
        "### 輔助函式：找出張量的形狀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyI25qhGPfT"
      },
      "source": [
        "from typing import List\r\n",
        "\r\n",
        "def shape(tensor:Tensor) ->List[int]:\r\n",
        "  sizes:List[int] = []\r\n",
        "  while isinstance(tensor,list):\r\n",
        "    sizes.append(len(tensor))\r\n",
        "    tensor = tensor[0]\r\n",
        "  return sizes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qb4-AEsazDh"
      },
      "source": [
        "assert shape([1,2,3]) == [3]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXsIC8XsazJ8"
      },
      "source": [
        "assert shape([[1,2],[3,4],[5,6]]) == [3,2]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7AGAv0fbnqe"
      },
      "source": [
        "由於張量可能具有任意數量的維度，  \r\n",
        "因此在使用張量時，通常需要採用**遞迴**的作法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xXAhkwzcQRo"
      },
      "source": [
        "def is_1d(tensor:Tensor) ->bool:\r\n",
        "  \"\"\"\r\n",
        "  如果tensor[0]是一個列表，他就是一個高維張量\r\n",
        "  否則就是一個一維向量\r\n",
        "  \"\"\"\r\n",
        "  return not isinstance(tensor[0],list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQBHGayKfGlt"
      },
      "source": [
        "assert is_1d([1,2,3])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QqTfOGDfMgf"
      },
      "source": [
        "assert not is_1d([[1,2],[3,4]])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An2tSSvMYxou"
      },
      "source": [
        "### 輔助函式：tensor_sum 函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAp2fHb1h1Tu"
      },
      "source": [
        "def tensor_sum(tensor:Tensor)->float:\r\n",
        "  \"\"\"把張量的所有值加總起來\"\"\"\r\n",
        "  if is_1d(tensor):\r\n",
        "    return sum(tensor) #只是一個浮點數列表，就用Python的sum函式\r\n",
        "  else:\r\n",
        "    return sum(tensor_sum(tensor_i) for tensor_i in tensor)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz3kN599ipib"
      },
      "source": [
        "assert tensor_sum([1,2,3]) == 6"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplU-Hygipl6"
      },
      "source": [
        "assert tensor_sum([[1,2],[3,4]]) == 10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZPRdxx2i_m3"
      },
      "source": [
        "### 輔助函式：把某個函數套用到單一張量中的每個元素"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6QLhWB_jTNN"
      },
      "source": [
        "from typing import Callable\r\n",
        "\r\n",
        "def tensor_apply(f:Callable[[float],float],tensor:Tensor) ->Tensor:\r\n",
        "  \"\"\"把函式f套用到每個元素\"\"\"\r\n",
        "  if is_1d(tensor):\r\n",
        "    return [f(x) for x in tensor]\r\n",
        "  else:\r\n",
        "    return [tensor_apply(f,tensor_i) for tensor_i in tensor]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TLvyl-bq7pL"
      },
      "source": [
        "assert tensor_apply(lambda x:x+1,[1,2,3]) == [2,3,4]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D09dMuvMq7xm"
      },
      "source": [
        "assert tensor_apply(lambda x:2*x,[[1,2],[3,4]]) == [[2,4],[6,8]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmGXgbgRZFLh"
      },
      "source": [
        "### 輔助函式：依據張量形狀，建立另一個形狀一樣的零張量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PSDSY2fZFXx"
      },
      "source": [
        "def zero_like(tensor:Tensor) ->Tensor:\r\n",
        "  return tensor_apply(lambda _: 0.0,tensor)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a87CsAo_ceaQ"
      },
      "source": [
        "assert zero_like([1,2,3]) == [0,0,0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYPFd8JQcefc"
      },
      "source": [
        "assert zero_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4XgTY9yczxt"
      },
      "source": [
        "### 輔助函式：將某個函式套用到兩個張量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nM-Tj84czxu"
      },
      "source": [
        "def tensor_combine(f:Callable[[float,float],float],t1:Tensor,t2:Tensor) ->Tensor:\r\n",
        "  \"\"\"把函式f套用到t1與t2的相應元素\"\"\"\r\n",
        "  if is_1d(t1):\r\n",
        "    return [f(x,y) for x,y in zip(t1,t2)]\r\n",
        "  else:\r\n",
        "    return [tensor_combine(f,t1_i,t2_i) for t1_i,t2_i in zip(t1,t2)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnb9TX7zczxv"
      },
      "source": [
        "import operator\r\n",
        "assert tensor_combine(operator.add,[1,2,3],[4,5,6]) == [5,7,9]\r\n",
        "assert tensor_combine(operator.mul,[1,2,3],[4,5,6]) == [4,10,18]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fT8zSoVUX4P"
      },
      "source": [
        "# 層的抽象概念\r\n",
        "\r\n",
        "建立一種機制，能用來時做出各式各樣的神經網路。\r\n",
        "最基本的概念是「Layer」。  \r\n",
        "他知道如何把輸入套入某種函數，亦能夠進行**反向傳播**的方式計算梯度。\r\n",
        "  \r\n",
        "在真正的子類別中，forward 與 backward 方法都會進行實作，  \r\n",
        "一旦建構了神經網路，我們就能以梯度遞減的方式來進行訓練。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDMQu5srquHd"
      },
      "source": [
        "from typing import Iterable,Tuple\r\n",
        "\r\n",
        "class Layer:\r\n",
        "  \"\"\"\r\n",
        "  我們的神經網路是由許多層組成，其中每一層都知道\r\n",
        "  如何以正向傳播的方式對輸入進行某些計算\r\n",
        "  以及如何以反向傳播的方式計算梯度\r\n",
        "  \"\"\"\r\n",
        "  def forward(self,input):\r\n",
        "    \"\"\"\r\n",
        "    可以注意到，這裡缺了型別可以設定。\r\n",
        "    我們並不打算限定各層的輸入是什麼型別。\r\n",
        "    也不限定return的輸入是甚麼型別\r\n",
        "    \"\"\"\r\n",
        "    raise NotImplementedError\r\n",
        "  \r\n",
        "  def backward(self,gradient):\r\n",
        "    \"\"\"\r\n",
        "    同樣地，我們並不打算限定梯度應該是什麼型別，\r\n",
        "    這完全由你自己決定，\r\n",
        "    只要確定合理即可\r\n",
        "    \"\"\"\r\n",
        "    raise NotImplementedError\r\n",
        "  \r\n",
        "  def params(self)->Iterable[Tensor]:\r\n",
        "    \"\"\"\r\n",
        "    返回此層的參數，預設的實作方式不會回送任何東西。\r\n",
        "    如果你的Layer沒有任何參數，\r\n",
        "    並不需要實做這個方法\r\n",
        "    \"\"\"\r\n",
        "    return ()\r\n",
        "  def grads(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"\r\n",
        "    送回梯度，順序與params相同\r\n",
        "    \"\"\"\r\n",
        "    return ()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3kdcqT4qTJE"
      },
      "source": [
        "### 範例：不須更新參數的sigmoid層：\r\n",
        "在正向傳遞中，保存了sigmoid函式的計算結果。  \r\n",
        "以供後續的反向傳遞使用。\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FXO4EaYUcBM"
      },
      "source": [
        "from typing import List\r\n",
        "import math\r\n",
        "\r\n",
        "Vector = List[float]\r\n",
        "\r\n",
        "def sigmoid(t:float)->float:\r\n",
        "  return 1/(1+math.exp(-t))\r\n",
        "\r\n",
        "class Sigmoid(Layer):\r\n",
        "\r\n",
        "  def forward(self,input:Tensor)->Tensor:\r\n",
        "    \"\"\"\r\n",
        "    把sigmoid函式套用到輸入張量的每個元素，\r\n",
        "    然後儲存結果，供反向傳播使用\r\n",
        "    \"\"\"\r\n",
        "    self.sigmoids = tensor_apply(sigmoid,input)\r\n",
        "    return self.sigmoids\r\n",
        "  \r\n",
        "  def backward(self,gradient:Tensor)->Tensor:\r\n",
        "    return tensor_combine(\r\n",
        "        lambda sig,grad:sig*(1-sig)*grad,\r\n",
        "        self.sigmoids,\r\n",
        "        gradient)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBbLJbvHUcmy"
      },
      "source": [
        "## 線性層\r\n",
        "代表神經元的 **dot(weight,inputs)** 點積的部分\r\n",
        "  \r\n",
        "在這裡實作三種方式生成初始的隨機張量：\r\n",
        "1. 自[0,1]的隨機分布中選擇初始值\r\n",
        "2. 自標準常態分布中選擇初始值\r\n",
        "3. 使用「Xavier initialization」，其中每個權重為平均是0，變異量為2/(num_inputs+num_outputs)的標準常態分布中選擇初始值\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHX0GNIrUTVZ"
      },
      "source": [
        "### 前置作業"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKo_NO-0Ueov"
      },
      "source": [
        "import random\r\n",
        "import math\r\n",
        "\r\n",
        "SQRT_TWO_PI = math.sqrt(2* math.pi)\r\n",
        "\r\n",
        "def normal_pdf(x:float,mu:float=0,sigma:float=1) ->float:\r\n",
        "  return (math.exp(-(x-mu)**2/2/sigma**2)/(SQRT_TWO_PI*sigma))\r\n",
        "\r\n",
        "def normal_cdf(x:float,mu:float=0,sigma:float=1)->float:\r\n",
        "  return (1+math.erf((x-mu)/math.sqrt(2)/sigma))/2\r\n",
        "\r\n",
        "def inverse_normal_cdf(p:float,\r\n",
        "            mu:float = 0,\r\n",
        "            sigma:float=1,\r\n",
        "            tolerance:float=0.00001) -> float:\r\n",
        "  # 如果不是標準常態分佈，就先轉換成標準常態分佈\r\n",
        "  if mu != 0 or sigma != 1:\r\n",
        "    return mu + sigma*inverse_normal_cdf(p,tolerance = tolerance)\r\n",
        "  \r\n",
        "  low_z = -10.0 # normal_cdf(-10)是(趨近於) 0\r\n",
        "  hi_z =  10.0 # normal_cdf(10)是(趨近於) 1\r\n",
        "  while hi_z - low_z > tolerance:  \r\n",
        "    mid_z = (low_z + hi_z) / 2   # 計算出中間值\r\n",
        "    mid_p = normal_cdf(mid_z)     # 以及累積分佈函數在該處所應對的值\r\n",
        "    if mid_p < p :\r\n",
        "      low_z = mid_z        #中間的值太低，就往上繼續搜尋\r\n",
        "    else:\r\n",
        "      hi_z = mid_z        #中間的值太高，就往下繼續搜尋\r\n",
        "  \r\n",
        "  return mid_z"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW2Vn3UhJ_Qz"
      },
      "source": [
        "def random_uniform(*dims:int) ->Tensor:\r\n",
        "  if len(dims) == 1:\r\n",
        "    return [random.random() for _ in range(dims[0])]\r\n",
        "  else:\r\n",
        "    return [random_uniform(*dims[1:]) for _ in range(dims[0])]\r\n",
        "\r\n",
        "def random_normal(*dims:int,mean:float=0.0,variance:float=1.0) ->Tensor:\r\n",
        "  if len(dims) == 1:\r\n",
        "    return [mean + variance * inverse_normal_cdf(random.random()) for _ in range(dims[0])]\r\n",
        "  else:\r\n",
        "    return [random_normal(*dims[1:],mean=mean,variance=variance) for _ in range(dims[0])]\r\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mbIHyPPMWif"
      },
      "source": [
        "assert shape(random_uniform(2,3,4)) == [2,3,4]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CncpfLMWlv"
      },
      "source": [
        "assert shape(random_normal(5,6,mean=10)) == [5,6]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY4zE-U3TZLi"
      },
      "source": [
        "將他們包裝在random_tensor函式中"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VBwpJI8Tfrw"
      },
      "source": [
        "def random_tensor(*dims:int,init:str = 'normal')->Tensor:\r\n",
        "  if init == 'normal':\r\n",
        "    return random_normal(*dims)\r\n",
        "  elif init == 'unifrom':\r\n",
        "    return random_uniform(*dims)\r\n",
        "  elif init == 'xavier':\r\n",
        "    variance = len(dims)/sum(dims)\r\n",
        "    return random_normal(*dims,variance=variance)\r\n",
        "  else:\r\n",
        "    raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z1E58LWUQMe"
      },
      "source": [
        "### 定義線性層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISoxN0F1UV5U"
      },
      "source": [
        "def dot(v:Vector,w:Vector)->float:\r\n",
        "  #計算v_1*w_1+... +v_n*w_n\r\n",
        "  assert len(v)==len(w),\"兩個向量必須有相同的維度\"\r\n",
        "\r\n",
        "  return sum(v_i*w_i for v_i,w_i in zip(v,w))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeEdoaCcUfMr"
      },
      "source": [
        "class Linear(Layer):\r\n",
        "  def __init__(self,input_dim:int,output_dim:int,init:str='xavier')->None:\r\n",
        "    \"\"\"\r\n",
        "    此層具有output_dims個神經元，每個神經元都有 imnput_dims個權重以及一個偏差量\r\n",
        "    \"\"\"\r\n",
        "    self.input_dim = input_dim\r\n",
        "    self.output_dim = output_dim\r\n",
        "\r\n",
        "    # self.w[o] 是第 o 個神經元的權重\r\n",
        "    self.w = random_tensor(output_dim,input_dim,init=init)\r\n",
        "\r\n",
        "    # self.w[o] 是第 o 個神經元的偏差項\r\n",
        "    self.b = random_tensor(output_dim,init=init)\r\n",
        "\r\n",
        "  def forward (self,input:Tensor) ->Tensor:\r\n",
        "      \r\n",
        "    \"\"\"\r\n",
        "    每個神經元都有一個輸出，將它保存在向量中\r\n",
        "    = 輸入和權重的點積 + 偏差值\r\n",
        "    \"\"\"\r\n",
        "    #保存以供反向傳遞使用\r\n",
        "    self.input = input\r\n",
        "\r\n",
        "    #送回神經元的輸出向量\r\n",
        "    return [dot(input,self.w[o])+self.b[o] for o in range(self.output_dim)]\r\n",
        "\r\n",
        "  def backward(self,gradient:Tensor) ->Tensor:\r\n",
        "    #　每個　b[o]　都會被加到　output[o]　之中\r\n",
        "    #　亦即ｂ的梯度等於　output的梯度\r\n",
        "    self.b_grad = gradient\r\n",
        "\r\n",
        "    # 每個w[o][i]都會乘以input[i]再加到output[o]之中\r\n",
        "    # 因此梯度是 input[i]*gradient[o]\r\n",
        "    self.w_grad = [[self.input[i]*gradient[o] for i in range(self.input_dim)] for o in range(self.output_dim)]\r\n",
        "\r\n",
        "    # 每個input[i]都會與每個w[o][i]相乘，再加到每個output[o]之中\r\n",
        "    # 因此其梯度就是橫跨所有outputs\r\n",
        "    # w[o][i]*gradient[o]加總之和\r\n",
        "    return [sum(self.w[o][i]* gradient[o] for o in range(self.output_dim)) for i in range(self.input_dim)]\r\n",
        "  \r\n",
        "  def paramas(self) ->Iterable[Tensor]:\r\n",
        "    return [self.w,self.b]\r\n",
        "  \r\n",
        "  def grads(self)->Iterable[Tensor]:\r\n",
        "    return [self.w_grad,self.b_grad]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvIcc97hUfl6"
      },
      "source": [
        "# 把神經網路視為一系列的層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmVWBY0xUlST"
      },
      "source": [
        "from typing import List\r\n",
        "\r\n",
        "class Sequential(Layer):\r\n",
        "  \"\"\"\r\n",
        "  一行就包括一系列的其他層。\r\n",
        "  每一層的輸出做為下一層的輸入，\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self,layers:List[Layer])->None:\r\n",
        "    self.layers = layers\r\n",
        "  \r\n",
        "  def forward(self,input):\r\n",
        "    \"\"\"讓輸入照順序正向通過每一層\"\"\"\r\n",
        "    for layer in self.layers:\r\n",
        "      input = layer.forward(input)\r\n",
        "    return input\r\n",
        "  \r\n",
        "  def backward(self,gradient):\r\n",
        "    \"\"\"讓梯度反向通過每一層以進行反向傳播\"\"\"\r\n",
        "    for layer in reversed(self.layers):\r\n",
        "      gradient = layer.backward(gradient)\r\n",
        "    return gradient\r\n",
        "  \r\n",
        "  def params(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"送回每一層的參數\"\"\"\r\n",
        "    return (param for layer in self.layers for param in layer.params())\r\n",
        "\r\n",
        "  def grads(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"送回每一層的梯度\"\"\"\r\n",
        "    return (grad for layer in self.layers for grad in layer.grads())"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW9Dara_HLjR"
      },
      "source": [
        "將XOR神經網路表示為："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxIzGM4YHTD0"
      },
      "source": [
        "xor_net=Sequential([\r\n",
        "  Linear(input_dim=2,output_dim=2),\r\n",
        "  Sigmoid(),\r\n",
        "  Linear(input_dim=2,output_dim=1),\r\n",
        "  Sigmoid()])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSXW6lccUmge"
      },
      "source": [
        "# 損失與最佳化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7VZXbxRMYXH"
      },
      "source": [
        "### 定義名為「Loss」的抽象類別"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67lJO9pWUoWz"
      },
      "source": [
        "class Loss:\r\n",
        "  def loss(self,predicted:Tensor,actual:Tensor) ->float:\r\n",
        "    \"\"\"計算我們預測結果的程度\"\"\"\r\n",
        "    raise NotImplementedError\r\n",
        "  \r\n",
        "  def gradient(self,predicted:Tensor,actual:Tensor) ->Tensor:\r\n",
        "    \"\"\"如果預測改變，則損失會隨之改變\"\"\"\r\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akk4XbMjMdX0"
      },
      "source": [
        "### 利用**平方誤差和**做為損失函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grM1_A85Mc4I"
      },
      "source": [
        "class SSE(Loss):\r\n",
        "  \"\"\"計算平方誤差和，以作為損失函數\"\"\"\r\n",
        "  def loss(self,predicted:Tensor,actual:Tensor)->float:\r\n",
        "\r\n",
        "    #計算出平方誤差量\r\n",
        "    squared_errors = tensor_combine(\r\n",
        "      lambda predicted,actual:(predicted - actual) ** 2,\r\n",
        "      predicted,\r\n",
        "      actual)\r\n",
        "\r\n",
        "    #接著全部加總起來\r\n",
        "    return tensor_sum(squared_errors)\r\n",
        "  \r\n",
        "  def gradient(self,predicted:Tensor,actual:Tensor)->Tensor:\r\n",
        "    return tensor_combine(\r\n",
        "      lambda predicted,actual: 2 * (predicted - actual),\r\n",
        "      predicted,\r\n",
        "      actual)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsCqNy6MdBv"
      },
      "source": [
        "### 梯度遞減的實作\r\n",
        "以往是以人工方式完成所有梯度遞減的操作，然而在這裡不太適用。\r\n",
        "1. 神經網路有多個參數需要更新\r\n",
        "2. 希望能使用更聰明的梯度遞減變形作法而無須重新改寫程式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITBGvoV7XDX7"
      },
      "source": [
        "class Optimizer:\r\n",
        "  \"\"\"\r\n",
        "  Optimizer會更新layer的權重值，\r\n",
        "  其根據不是來自layer就是來自Optimizer\r\n",
        "  \"\"\"\r\n",
        "  def step(self,layer:Layer) ->None:\r\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4btsn8qbPfD"
      },
      "source": [
        "利用Optimizer來實作梯度遞減\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF-eA1FDbPua"
      },
      "source": [
        "class GradientDescent(Optimizer):\r\n",
        "  def __init__(self,learning_rate:float=0.1)->None:\r\n",
        "    self.lr = learning_rate\r\n",
        "  \r\n",
        "  def step(self,layer:Layer) ->None:\r\n",
        "    for param,grad in zip(layer.params(),layer.grads()):\r\n",
        "      #運用梯度遞減的方式更新參數\r\n",
        "      param[:] = tensor_combine(\r\n",
        "        lambda param,grad:param - grad * self.lr,\r\n",
        "        param,\r\n",
        "        grad\r\n",
        "      )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7pAg2xPdtgr"
      },
      "source": [
        "### 片段指定值(slice assignment)\r\n",
        "**反映一個事實**：重新指定列表的值，並不會改變其值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFFpnqgXd-sm"
      },
      "source": [
        "tensor= [[1,2],[3,4]]\r\n",
        "\r\n",
        "for row in tensor:\r\n",
        "  row = [0,0]\r\n",
        "\r\n",
        "assert tensor == [[1,2],[3,4]] #一般指定值的方式不會更動到列表內容\r\n",
        "\r\n",
        "for row in tensor:\r\n",
        "  row[:] = [0,0]\r\n",
        "\r\n",
        "assert tensor == [[0,0],[0,0]] #  片段指定值的方式就會更動到列表內容"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KalOzNqqkiBs"
      },
      "source": [
        "### 動量(momentum)\r\n",
        "保留先前梯度移動平均值，每得到新的梯度值就更新這個平均梯度值並往這個平均方向邁開一步。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ4wJPlAoGMz"
      },
      "source": [
        "class Momentum(Optimizer):\r\n",
        "  def __init__(self,learning_rate = float,momentum:float=0.9) ->None:\r\n",
        "    self.lr = learning_rate\r\n",
        "    self.mo = momentum\r\n",
        "    self.updates:List[Tensor]=[] #移動平均\r\n",
        "\r\n",
        "  def step(self,layer:Layer)->None:\r\n",
        "    #如果還沒有之前的更新值，就全部使用零來當作起始值\r\n",
        "    if not self.updates:\r\n",
        "      self.updates = [zero_like(grad) for grad in layer.grads()]\r\n",
        "    \r\n",
        "    for update,param,grad in zip(self.updates,layer.params(),layer.grads()):\r\n",
        "      #套用動量的計算方式\r\n",
        "      update[:] = tensor_combine(lambda u,g:self.mo*u + (1-self.mo)*g, update , grad)\r\n",
        "\r\n",
        "      #接著執行梯度遞減的步驟\r\n",
        "      param[:] = tensor_combine(lambda p,u:p-self.lr*u,param,update)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaUMuzB5UpHZ"
      },
      "source": [
        "# 範例：XOR再次嘗試"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTyx0vvHqET0"
      },
      "source": [
        "重新建立訓練資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SB9e9PLUtL-"
      },
      "source": [
        "xs = [[0.,0],[0.,1],[1.,0],[1.,1]]\r\n",
        "ys = [[0.],[1.],[1.],[0.]]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4BRthc5qYs-"
      },
      "source": [
        "定義網路  \r\n",
        "(*在這裡可以省去最後的sigmoid層*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IbqiiuaqaRg"
      },
      "source": [
        "random.seed(0)\r\n",
        "\r\n",
        "net = Sequential([\r\n",
        "  Linear(input_dim=2,output_dim=2),\r\n",
        "  Sigmoid(),\r\n",
        "  Linear(input_dim=2,output_dim=1)\r\n",
        "])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZeRrgzyo7Zt"
      },
      "source": [
        "定義一個簡易的定義迴圈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWUoTk9o7hz",
        "outputId": "2d89b235-8a8d-401a-e938-229ecd32860f"
      },
      "source": [
        "import tqdm\r\n",
        "\r\n",
        "optimizer = GradientDescent(learning_rate=0.1)\r\n",
        "loss = SSE()\r\n",
        "\r\n",
        "with tqdm.trange(3000) as t:\r\n",
        "  for epoch in t:\r\n",
        "    epoch_loss = 0.0\r\n",
        "\r\n",
        "    for x,y in zip(xs,ys):\r\n",
        "      predicted = net.forward(x)\r\n",
        "      #print(predicted)\r\n",
        "      epoch_loss += loss.loss(predicted,y)\r\n",
        "      gradient = loss.gradient(predicted,y)\r\n",
        "      net.backward(gradient)\r\n",
        "\r\n",
        "      optimizer.step(net)\r\n",
        "    \r\n",
        "    t.set_description(f\"xor loss {epoch_loss:.3f}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xor loss 1.597: 100%|██████████| 3000/3000 [00:03<00:00, 751.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hgt41Bk3log"
      },
      "source": [
        "檢查訓練後的網路"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfLUJHj13pED"
      },
      "source": [
        "for param in net.params():\r\n",
        "  print(param)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZdweO51nqdq"
      },
      "source": [
        "從得到的網路來看，結果大致如下：\r\n",
        "* hidden1 = -2.6 * x1 + -2.7 * x2 + 0.2\r\n",
        "* hidden2 = 2.1 * x1 + 2.1 * x2 -3.4\r\n",
        "* optput= -3.1 * h1 + -2.6 * x2 + 1.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnijD7kSUxY4"
      },
      "source": [
        "# 其他激活函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN2IrAL4U0Wl"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcHTbFtU1UQ"
      },
      "source": [
        "# **範例**：FizzBuzz 再次嘗試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR6KOXHsU-OG"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH64QUuLU-cM"
      },
      "source": [
        "# Softmax與交叉熵"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_MJ-jjVLRb"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6CQrGMVGbu"
      },
      "source": [
        "# Dropout隨機拋棄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIIcHKXVLxK"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f84htktVNAg"
      },
      "source": [
        "# 範例：MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lx5MJOVPmz"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPdggxWVS5F"
      },
      "source": [
        "# 模型的儲存與載入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxCBiOc9VW6i"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}