{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter19.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMti9DZIl6PqhpnFhh4bmjf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hank199599/data_science_from_scratch_reading_log/blob/main/Chapter19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-upQJRhBT4RG"
      },
      "source": [
        "# 深度學習(Deep learning)\r\n",
        "原本指的是「深度」神經網路，現被用來泛指各種神經網路。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDNDb28zUSny"
      },
      "source": [
        "# 張量\r\n",
        "在神經網路函示庫中，n維陣列被稱為**張量(tensor)**  \r\n",
        "理想情況下，可以使用：\r\n",
        "\r\n",
        "```python\r\n",
        "# 張量Tensor要不是一個浮點數，就是一個張量列表\r\n",
        "Tensor = Union[float,List[Tensor]]\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woz6w24ma8-S"
      },
      "source": [
        "#如同我們說：\r\n",
        "Tensor = list"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd7FNN-kYo3Q"
      },
      "source": [
        "### 輔助函式：找出張量的形狀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyI25qhGPfT"
      },
      "source": [
        "from typing import List\r\n",
        "\r\n",
        "def shape(tensor:Tensor) ->List[int]:\r\n",
        "  sizes:List[int] = []\r\n",
        "  while isinstance(tensor,list):\r\n",
        "    sizes.append(len(tensor))\r\n",
        "    tensor = tensor[0]\r\n",
        "  return sizes"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qb4-AEsazDh"
      },
      "source": [
        "assert shape([1,2,3]) == [3]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXsIC8XsazJ8"
      },
      "source": [
        "assert shape([[1,2],[3,4],[5,6]]) == [3,2]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7AGAv0fbnqe"
      },
      "source": [
        "由於張量可能具有任意數量的維度，  \r\n",
        "因此在使用張量時，通常需要採用**遞迴**的作法。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xXAhkwzcQRo"
      },
      "source": [
        "def is_1d(tensor:Tensor) ->bool:\r\n",
        "  \"\"\"\r\n",
        "  如果tensor[0]是一個列表，他就是一個高維張量\r\n",
        "  否則就是一個一維向量\r\n",
        "  \"\"\"\r\n",
        "  return not isinstance(tensor[0],list)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQBHGayKfGlt"
      },
      "source": [
        "assert is_1d([1,2,3])"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QqTfOGDfMgf"
      },
      "source": [
        "assert not is_1d([[1,2],[3,4]])"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An2tSSvMYxou"
      },
      "source": [
        "### 輔助函式：tensor_sum 函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAp2fHb1h1Tu"
      },
      "source": [
        "def tensor_sum(tensor:Tensor)->float:\r\n",
        "  \"\"\"把張量的所有值加總起來\"\"\"\r\n",
        "  if is_1d(tensor):\r\n",
        "    return sum(tensor) #只是一個浮點數列表，就用Python的sum函式\r\n",
        "  else:\r\n",
        "    return sum(tensor_sum(tensor_i) for tensor_i in tensor)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz3kN599ipib"
      },
      "source": [
        "assert tensor_sum([1,2,3]) == 6"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nplU-Hygipl6"
      },
      "source": [
        "assert tensor_sum([[1,2],[3,4]]) == 10"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZPRdxx2i_m3"
      },
      "source": [
        "### 輔助函式：把某個函數套用到單一張量中的每個元素"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6QLhWB_jTNN"
      },
      "source": [
        "from typing import Callable\r\n",
        "\r\n",
        "def tensor_apply(f:Callable[[float],float],tensor:Tensor) ->Tensor:\r\n",
        "  \"\"\"把函式f套用到每個元素\"\"\"\r\n",
        "  if is_1d(tensor):\r\n",
        "    return [f(x) for x in tensor]\r\n",
        "  else:\r\n",
        "    return [tensor_apply(f,tensor_i) for tensor_i in tensor]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TLvyl-bq7pL"
      },
      "source": [
        "assert tensor_apply(lambda x:x+1,[1,2,3]) == [2,3,4]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D09dMuvMq7xm"
      },
      "source": [
        "assert tensor_apply(lambda x:2*x,[[1,2],[3,4]]) == [[2,4],[6,8]]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmGXgbgRZFLh"
      },
      "source": [
        "### 輔助函式：依據張量形狀，建立另一個形狀一樣的零張量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PSDSY2fZFXx"
      },
      "source": [
        "def zero_like(tensor:Tensor) ->Tensor:\r\n",
        "  return tensor_apply(lambda _: 0.0,tensor)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a87CsAo_ceaQ"
      },
      "source": [
        "assert zero_like([1,2,3]) == [0,0,0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYPFd8JQcefc"
      },
      "source": [
        "assert zero_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4XgTY9yczxt"
      },
      "source": [
        "### 輔助函式：將某個函式套用到兩個張量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nM-Tj84czxu"
      },
      "source": [
        "def temsor_combine(f:Callable[[float,float],float],t1:Tensor,t2:Tensor) ->Tensor:\r\n",
        "  \"\"\"把函式f套用到t1與t2的相應元素\"\"\"\r\n",
        "  if is_1d(t1):\r\n",
        "    return [f(x,y) for x,y in zip(t1,t2)]\r\n",
        "  else:\r\n",
        "    return [temsor_combine(f,t1_i,t2_i) for t1_i,t2_i in zip(t1,t2)]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnb9TX7zczxv"
      },
      "source": [
        "import operator\r\n",
        "assert temsor_combine(operator.add,[1,2,3],[4,5,6]) == [5,7,9]\r\n",
        "assert temsor_combine(operator.mul,[1,2,3],[4,5,6]) == [4,10,18]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fT8zSoVUX4P"
      },
      "source": [
        "# 層的抽象概念\r\n",
        "\r\n",
        "建立一種機制，能用來時做出各式各樣的神經網路。\r\n",
        "最基本的概念是「Layer」。  \r\n",
        "他知道如何把輸入套入某種函數，亦能夠進行**反向傳播**的方式計算梯度。\r\n",
        "  \r\n",
        "在真正的子類別中，forward 與 backward 方法都會進行實作，  \r\n",
        "一旦建構了神經網路，我們就能以梯度遞減的方式來進行訓練。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDMQu5srquHd"
      },
      "source": [
        "from typing import Iterable,Tuple\r\n",
        "\r\n",
        "class Layer:\r\n",
        "  \"\"\"\r\n",
        "  我們的神經網路是由許多層組成，其中每一層都知道\r\n",
        "  如何以正向傳播的方式對輸入進行某些計算\r\n",
        "  以及如何以反向傳播的方式計算梯度\r\n",
        "  \"\"\"\r\n",
        "  def forward(self,input):\r\n",
        "    \"\"\"\r\n",
        "    可以注意到，這裡缺了型別可以設定。\r\n",
        "    我們並不打算限定各層的輸入是什麼型別。\r\n",
        "    也不限定return的輸入是甚麼型別\r\n",
        "    \"\"\"\r\n",
        "    raise NotImplementedError\r\n",
        "  \r\n",
        "  def backward(self,gradient):\r\n",
        "    \"\"\"\r\n",
        "    同樣地，我們並不打算限定梯度應該是什麼型別，\r\n",
        "    這完全由你自己決定，\r\n",
        "    只要確定合理即可\r\n",
        "    \"\"\"\r\n",
        "    raise NotImplementedError\r\n",
        "  \r\n",
        "  def params(self)->Iterable[Tensor]:\r\n",
        "    \"\"\"\r\n",
        "    返回此層的參數，預設的實作方式不會回送任何東西。\r\n",
        "    如果你的Layer沒有任何參數，\r\n",
        "    並不需要實做這個方法\r\n",
        "    \"\"\"\r\n",
        "    return ()\r\n",
        "  def grads(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"\r\n",
        "    送回梯度，順序與params相同\r\n",
        "    \"\"\"\r\n",
        "    return ()"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3kdcqT4qTJE"
      },
      "source": [
        "### 範例：不須更新參數的sigmoid層："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FXO4EaYUcBM"
      },
      "source": [
        "from typing import List\r\n",
        "import math\r\n",
        "\r\n",
        "Vector = List[float]\r\n",
        "\r\n",
        "def sigmoid(t:float)->float:\r\n",
        "  return 1/(1+math.exp(-t))\r\n",
        "\r\n",
        "class Sigmoid(Layer):\r\n",
        "\r\n",
        "  def forward(self,input:Vector)->Tensor:\r\n",
        "    \"\"\"\r\n",
        "    把sigmoid函式套用到輸入張量的每個元素，\r\n",
        "    然後儲存結果，供反向傳播使用\r\n",
        "    \"\"\"\r\n",
        "    self.sigmoids = tensor_apply(sigmoid,input)\r\n",
        "    return self.sigmoids\r\n",
        "  \r\n",
        "  def backward(self,gradient:Tensor)->Tensor:\r\n",
        "    return tensor_combine(lambda sig,grad:sig*(1-sig)*grad,self.sigmoids,gradient)\r\n"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBbLJbvHUcmy"
      },
      "source": [
        "## 線性層\r\n",
        "代表神經元的 **dot(weight,inputs)** 點積的部分\r\n",
        "  \r\n",
        "在這裡實作三種方式生成初始的隨機張量：\r\n",
        "1. 自[0,1]的隨機分布中選擇初始值\r\n",
        "2. 自標準常態分布中選擇初始值\r\n",
        "3. 使用「Xavier initialization」，其中每個權重為平均是0，變異量為2/(num_inputs+num_outputs)的標準常態分布中選擇初始值\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHX0GNIrUTVZ"
      },
      "source": [
        "### 前置作業"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKo_NO-0Ueov"
      },
      "source": [
        "import random\r\n",
        "import math\r\n",
        "\r\n",
        "SQRT_TWO_PI = math.sqrt(2* math.pi)\r\n",
        "\r\n",
        "def normal_pdf(x:float,mu:float=0,sigma:float=1) ->float:\r\n",
        "  return (math.exp(-(x-mu)**2/2/sigma**2)/(SQRT_TWO_PI*sigma))\r\n",
        "\r\n",
        "def normal_cdf(x:float,mu:float=0,sigma:float=1)->float:\r\n",
        "  return (1+math.erf((x-mu)/math.sqrt(2)/sigma))/2\r\n",
        "\r\n",
        "def inverse_normal_cdf(p:float,\r\n",
        "            mu:float = 0,\r\n",
        "            sigma:float=1,\r\n",
        "            tolerance:float=0.00001) -> float:\r\n",
        "  # 如果不是標準常態分佈，就先轉換成標準常態分佈\r\n",
        "  if mu != 0 or sigma != 1:\r\n",
        "    return mu + sigma*inverse_normal_cdf(p,tolerance = tolerance)\r\n",
        "  \r\n",
        "  low_z = -10.0 # normal_cdf(-10)是(趨近於) 0\r\n",
        "  hi_z =  10.0 # normal_cdf(10)是(趨近於) 1\r\n",
        "  while hi_z - low_z > tolerance:  \r\n",
        "    mid_z = (low_z + hi_z) / 2   # 計算出中間值\r\n",
        "    mid_p = normal_cdf(mid_z)     # 以及累積分佈函數在該處所應對的值\r\n",
        "    if mid_p < p :\r\n",
        "      low_z = mid_z        #中間的值太低，就往上繼續搜尋\r\n",
        "    else:\r\n",
        "      hi_z = mid_z        #中間的值太高，就往下繼續搜尋\r\n",
        "  \r\n",
        "  return mid_z"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW2Vn3UhJ_Qz"
      },
      "source": [
        "def random_uniform(*dims:int) ->Tensor:\r\n",
        "  if len(dims) == 1:\r\n",
        "    return [random.random() for _ in range(dims[0])]\r\n",
        "  else:\r\n",
        "    return [random_uniform(*dims[1:]) for _ in range(dims[0])]\r\n",
        "\r\n",
        "def random_normal(*dims:int,mean:float=0.0,variance:float=1.0) ->Tensor:\r\n",
        "  if len(dims) == 1:\r\n",
        "    return [mean + variance * inverse_normal_cdf(random.random()) for _ in range(dims[0])]\r\n",
        "  else:\r\n",
        "    return [random_normal(*dims[1:],mean=mean,variance=variance) for _ in range(dims[0])]\r\n",
        "  "
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mbIHyPPMWif"
      },
      "source": [
        "assert shape(random_uniform(2,3,4)) == [2,3,4]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CncpfLMWlv"
      },
      "source": [
        "assert shape(random_normal(5,6,mean=10)) == [5,6]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY4zE-U3TZLi"
      },
      "source": [
        "將他們包裝在random_tensor函式中"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VBwpJI8Tfrw"
      },
      "source": [
        "def random_tensor(*dims:int,init:str = 'normal')->Tensor:\r\n",
        "  if init == 'normal':\r\n",
        "    return random_normal(*dims)\r\n",
        "  elif init == 'unifrom':\r\n",
        "    return random_uniform(*dims)\r\n",
        "  elif init == 'xavier':\r\n",
        "    variance = len(dims)/sum(dims)\r\n",
        "    return random_normal(*dims,variance=variance)\r\n",
        "  else:\r\n",
        "    raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z1E58LWUQMe"
      },
      "source": [
        "### 定義線性層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISoxN0F1UV5U"
      },
      "source": [
        "def dot(v:Vector,w:Vector)->float:\r\n",
        "  #計算v_1*w_1+... +v_n*w_n\r\n",
        "  assert len(v)==len(w),\"兩個向量必須有相同的維度\"\r\n",
        "\r\n",
        "  return sum(v_i*w_i for v_i,w_i in zip(v,w))"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeEdoaCcUfMr"
      },
      "source": [
        "class Linear(Layer):\r\n",
        "  def __init__(self,input_dim:int,output_dim:int,init:str='xavier')->None:\r\n",
        "    \"\"\"\r\n",
        "    此層具有output_dims個神經元，每個神經元都有 imnput_dims個權重以及一個偏差量\r\n",
        "    \"\"\"\r\n",
        "    self.input_dim = input_dim\r\n",
        "    self.output_dim = output_dim\r\n",
        "\r\n",
        "    # self.w[o] 是第 o 個神經元的權重\r\n",
        "    self.w = random_tensor(output_dim,input_dim,init=init)\r\n",
        "\r\n",
        "    # self.w[o] 是第 o 個神經元的偏差項\r\n",
        "    self.b = random_tensor(output_dim,init=init)\r\n",
        "  \r\n",
        "  \"\"\"\r\n",
        "  每個神經元都有一個輸出，將它保存在向量中\r\n",
        "  = 輸入和權重的點積 + 偏差值\r\n",
        "  \"\"\"\r\n",
        "  def forward (self,input:Tensor) ->Tensor:\r\n",
        "    #保存以供反向傳遞使用\r\n",
        "    self.input = input\r\n",
        "\r\n",
        "    #送回神經元的輸出向量\r\n",
        "    return [dot(input,self.w[o])+self.b[o] for o in range(self.output_dim)]\r\n",
        "\r\n",
        "  def backward(self,gradient:Tensor) ->Tensor:\r\n",
        "    #　每個　b[o]　都會被加到　output[o]　之中\r\n",
        "    #　亦即ｂ的梯度等於　output的梯度\r\n",
        "    self.b_grad = gradient\r\n",
        "\r\n",
        "    # 每個w[o][i]都會乘以input[i]再加到output[o]之中\r\n",
        "    # 因此梯度是 input[i]*gradient[o]\r\n",
        "    self.w_grad = [[self.input[i]*gradient[o] for i in range(self.input_dim)] for o in range(self.output_dim)]\r\n",
        "\r\n",
        "    # 每個input[i]都會與每個w[o][i]相乘，再加到每個output[o]之中\r\n",
        "    # 因此其梯度就是橫跨所有outputs\r\n",
        "    # w[o][i]*gradient[o]加總之和\r\n",
        "    return [sum(self.w[o][i]* gradient[o] for o in range(self.output_dim)) for i in range(self.input_dim)]\r\n",
        "  \r\n",
        "  def paramas(self) ->Iterable[Tensor]:\r\n",
        "    return [self.w,self,b]\r\n",
        "  \r\n",
        "  def grads(self)->Iterable[Tensor]:\r\n",
        "    return [self.w_grad,self,b_grad]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvIcc97hUfl6"
      },
      "source": [
        "# 把神經網路視為一系列的層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmVWBY0xUlST"
      },
      "source": [
        "from typing import List\r\n",
        "\r\n",
        "def Sequential(Layer):\r\n",
        "  \"\"\"\r\n",
        "  一行就包括一系列的其他層。\r\n",
        "  每一層的輸出做為下一層的輸入，\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self,layers:List[Layer])->None:\r\n",
        "    self.layers = layers\r\n",
        "  \r\n",
        "  def forward(self,input):\r\n",
        "    \"\"\"讓輸入照順序正向通過每一層\"\"\"\r\n",
        "    for layer in self.layers:\r\n",
        "      input = layer.forward(input)\r\n",
        "    return input\r\n",
        "  \r\n",
        "  def backward(self,gradient):\r\n",
        "    \"\"\"讓梯度反向通過每一層以進行反向傳播\"\"\"\r\n",
        "    for layer in reversed(self.layers):\r\n",
        "      gradient = layer.backward(gradient)\r\n",
        "    return gradient\r\n",
        "  \r\n",
        "  def params(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"送回每一層的參數\"\"\"\r\n",
        "    return (param for layer in self.layers for param in layers.params())\r\n",
        "\r\n",
        "  def grads(self) ->Iterable[Tensor]:\r\n",
        "    \"\"\"送回每一層的梯度\"\"\"\r\n",
        "    return (grad for layer in self.layers for grad in layers.grads())"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW9Dara_HLjR"
      },
      "source": [
        "將XOR神經網路表示為："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "gxIzGM4YHTD0",
        "outputId": "c3a8b462-8f94-42b2-963d-c0302d8d4a44"
      },
      "source": [
        "xor_net=Sequential([\r\n",
        "  Linear(input_dim=2,output_dim=2),\r\n",
        "  Sigmoid(),\r\n",
        "  Linear(input_dim=2,output_dim=1),\r\n",
        "  Sigmoid()\r\n",
        "])"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-f81e4ea5aa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m ])\n",
            "\u001b[0;32m<ipython-input-180-b626a8e33589>\u001b[0m in \u001b[0;36mSequential\u001b[0;34m(Layer)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0m每一層的輸出做為下一層的輸入\u001b[0m\u001b[0;31m，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/typing.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mpass\u001b[0m  \u001b[0;31m# All real errors (not unhashable args) are raised below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/typing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \"Parameter list to %s[...] cannot be empty\" % _qualname(self))\n\u001b[1;32m   1106\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Parameters to generic types must be types.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Generic can only be subscripted with unique type variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/typing.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \"Parameter list to %s[...] cannot be empty\" % _qualname(self))\n\u001b[1;32m   1106\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Parameters to generic types must be types.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Generic can only be subscripted with unique type variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/typing.py\u001b[0m in \u001b[0;36m_type_check\u001b[0;34m(arg, msg)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TypingBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     ):\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Got %.100r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m     \u001b[0;31m# Bare Union etc. are not valid as type arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     if (\n",
            "\u001b[0;31mTypeError\u001b[0m: Parameters to generic types must be types. Got [<__main__.Linear object at 0x7fd39ab5f278>, <__main__.Sigmoid object at 0x7fd39ab5f0f0>, <__main__.."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSXW6lccUmge"
      },
      "source": [
        "# 損失與最佳化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67lJO9pWUoWz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaUMuzB5UpHZ"
      },
      "source": [
        "# 範例：XOR再次嘗試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SB9e9PLUtL-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnijD7kSUxY4"
      },
      "source": [
        "# 其他激活函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN2IrAL4U0Wl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcHTbFtU1UQ"
      },
      "source": [
        "# **範例**：FizzBuzz 再次嘗試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR6KOXHsU-OG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH64QUuLU-cM"
      },
      "source": [
        "# Softmax與交叉熵"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT_MJ-jjVLRb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6CQrGMVGbu"
      },
      "source": [
        "# Dropout隨機拋棄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIIcHKXVLxK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f84htktVNAg"
      },
      "source": [
        "# 範例：MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lx5MJOVPmz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPdggxWVS5F"
      },
      "source": [
        "# 模型的儲存與載入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxCBiOc9VW6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}