{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter8.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNcsczYH9h3h4fmi40LapuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hank199599/data_science_from_scratch_reading_log/blob/main/Chapter8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMtyZZeBlCfk"
      },
      "source": [
        "進行這專案前，需要引用第四章的向量定義以及相關運算來進行初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyI25qhGPfT"
      },
      "source": [
        "from typing import List\n",
        "import math\n",
        "\n",
        "Vector = List[float]\n",
        "\n",
        "def subtrate( v:Vector, w:Vector) -> Vector:\n",
        "  assert len(v) == len(w) #兩個向量必須有相同的維度\n",
        "\n",
        "  return [ v_i-w_i for v_i,w_i in zip(v,w)]\n",
        "\n",
        "def dot(v:Vector,w:Vector)->float:\n",
        "  #計算v_1*w_1+... +v_n*w_n\n",
        "  assert len(v)==len(w),\"兩個向量必須有相同的維度\"\n",
        "\n",
        "  return sum(v_i*w_i for v_i,w_i in zip(v,w))\n",
        "\n",
        "def sum_of_squares(v:Vector) -> float:\n",
        "  return dot(v,v)\n",
        "\n",
        "def squared_distance(v:Vector,w:Vector) -> float:\n",
        "  return sum_of_squares(subtrate(v,w))\n",
        "\n",
        "def distance(v:Vector,w:Vector) -> float:\n",
        "  return math.sqrt(squared_distance(v,w))\n",
        "\n",
        "def add( v:Vector, w:Vector) -> Vector:\n",
        "  assert len(v) == len(w) ,\"兩個向量必須有相同的維度\"\n",
        "\n",
        "  return [ v_i+w_i for v_i,w_i in zip(v,w)]\n",
        "\n",
        "def scalar_multiply(c:float,v:Vector) -> Vector:\n",
        "  return [c*v_i for v_i in v]\n",
        "\n",
        "def vector_mean(vectors:List[Vector])->Vector:\n",
        "  n=len(vectors)\n",
        "  return scalar_multiply(1/n,vector_sum(vectors))\n",
        "\n",
        "def vector_sum(vectors:List[Vector]) -> Vector:\n",
        "  #先檢查vertors這個向量列表是否為空\n",
        "  assert vectors,\"列表中沒有向量!\"\n",
        "\n",
        "  #檢查vertors 向量列表內的所有向量都具有相同的維度\n",
        "  num_elements=len(vectors[0])\n",
        "  assert all(len(v)==num_elements for v in vectors),\"向量維度不一致\"\n",
        "\n",
        "  #所有vectors[i]相加起來，是結果的第i個元素值\n",
        "  return [sum(vector[i] for vector in vectors) for i in range(num_elements)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejvodcbrkhK9"
      },
      "source": [
        "# 梯度遞減(gradient descent)\n",
        "用於找出某個輸入值v，使函數得出的最大值(或最小值)的可能值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COYuPhDDlPlE"
      },
      "source": [
        "def sum_of_square(v:Vector)->float:\n",
        "  \"\"\"計算v之中的平方和\"\"\"\n",
        "  return dot(v,v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zs16UC6lnvk"
      },
      "source": [
        "## 梯度(*gradient*)：向量的偏導數\n",
        "  \n",
        "欲求出函數的**最大值**：\n",
        "1. 取個隨機的起始點\n",
        "2. 計算其梯度\n",
        "3. 沿著梯度方向(即函數增長最多的方向)移動一小步\n",
        "\n",
        "欲求出函數的**最小值**： \n",
        "1. 取個隨機的起始點\n",
        "2. 計算其梯度\n",
        "3. 沿著梯度**反方向**移動一小步"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2f709NOnC3e"
      },
      "source": [
        "### 圖8-1  利用梯度遞減的概念，來找出最小值\n",
        "![pic 8-1](https://lh3.googleusercontent.com/25UnUOO0nEydtbTOO2oImdEs9gcZCRAV605Rcyj3sq7R1sJvhiAbrXCU4NOAmk7e2Fy13nqZr_sUtttgzGKjWukxsSyTtjXdPcERR-fQNODiKqjJJEzJkyDhweAINiXpE5DSI9u9pOIsctPLXFkum9tWDjil3td9ZuhrXAtQUzjXI6urrOARRVK0RTcve7QzKqKrkXUVESGc09MEX5aZ-WQzbClFJUInfZcyAF6p5AZd6RXw2f5-PfQ5d_LVAA97lukdnup3ZmvR5NaSBbBsoe40F8vPe3RdukmBAd-QfhluROmTQ38S5CKazKeINvzg91naEWsiKo_0AVuyS9PZVLg2kLEf7qGmyH5bo0vi__kwA3Jgu4MNw50PGJjOFWkVVVB9MVWbiUmMXoOebxLPkgFnIWRprkvMkj1dMc1HjB3A2TBT5fFS-3A3MzPOJ4DoVY8B939t5zkUAbBSpA83h8oFeepa2jwKahH-GavgLsmuq_U8OB0-n9TlfEMSgxFbJS01uyQ5_epjH12olp7prDN9vowxAHyW62GqdTK5HJDVQEa2ZdYK26pooJFUibhwf88ub7qsn-z3mrM1y4qro-087Drt97_AcctWW7uccCoEePXB3fpzKgIfkxztD4MGnsDaYfucJOTKD3I5UK3o7L8rMCIlMPuT_mxsi6Au618iDwYmEZMv7IszQ8LdUQ=w762-h559-no?authuser=0)\n",
        "  \n",
        "如果函數只有一個局域最小值，則我們能輕易找到這個值，但如果函數有好幾個極小值,就有可能找錯點。  \n",
        "如果函數根本沒有最小值，則會陷入無限迴圈。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP4yfRfTpEX8"
      },
      "source": [
        "# 梯度的估算\n",
        "\n",
        "如果f函數只有一個變數  \n",
        "在點X所謂的導數(derivative)就是衡量x出現微小變化時f(x)跟著變化的程度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaBX9girqPNE"
      },
      "source": [
        "導數可以定義為商差(difference quotient)的極限：  \n",
        "其中h趨近於0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvVdcSJEox28"
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def difference_quotient(f:Callable[[float],float],x:float,h:float)->float:\n",
        "  return (f(x+h)-f(x))/h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhwRr0WRpjBD"
      },
      "source": [
        "### 圖8-2 利用商差的定義來趨近導數的定義\n",
        "導數就是(x,f(x))這個點的切線斜率，  \n",
        "而商差則是穿越(x,f(x))和(x+h,f(x+h))這兩點的割線斜率。  \n",
        "隨著h趨近於0，割線與切線會越來越近。\n",
        "  \n",
        "![pic 8-2](https://lh3.googleusercontent.com/rbCDonyegkqDkirKhNAsGChU0Gd4LjZKC8BkG1intRmjxE0SPr0OG_25WBOS7hfW5vWvgIVkFN7VoYZN4jifGkobb7ycEhyPzSrXWjv1eGEZroC8c5PM5tMB_YO-7z7t2VnIymYTKClJAzSsLfe7hpgmIXVNSNIWi7YARrr34BiMQYrOBQzE5xUM-ZaN-sm9D5zOsiSSAsXvXO6JMknT6jlPrvUK-9LmbTFH6b5zMSv9hwpFbF-fE18Hyr6-Y9XqTa09ZVkzbrwkd_31VQLhovJZAA5uOcGJHnSfG_66HWXPhWQBXKcbBb143uCANyzIscyR5eAYBtoORlnlWchL3p0FeQ2QWebutOMQ0_VaOh1xbIjbfV7Mx17EeFQRGNv0gYhRwMM7RZRlsfSJbq_MVhTiLr0uASpqOq7Kk2ponK0MBvBVPK93Gh3hUcgu-CQj8toy3Xo9qBfqedxLzPy_0Yl0ZfHkA7dLXqUJNvdJU0T2q1s1lmcSP66t-bTid3GZtnSVqNi7jO3N3TPgW74AA1zepO5SbIo-6uzz7lK1_nk9StqWg2j1z0ovIezAQVieVGjB9TBOMct-RiCN31oLp5I0dIvN6KUQA0IxwUan9s0xuKmcc7ZmioHJQqfw3PYNqsAc1fmUz7a-dJ-iDp67b4AZa-ADVWXQgVD6ic1ymwjo-mcl08f3iMNPLGcIWQ=w971-h708-no?authuser=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRe1RG0Vq_89"
      },
      "source": [
        "## 範例：平方函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07koYVJlrCRr"
      },
      "source": [
        "#平方函數\n",
        "def square(x:float)->float:\n",
        "  return x*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVgIbutqrJOj"
      },
      "source": [
        "#平方函數的導數\n",
        "def devivative(x:float)->float:\n",
        "  return 2*x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksSzVY-2ralL"
      },
      "source": [
        "### 圖8-3 利用商差獲取導數的結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2q3vuxQrg2z"
      },
      "source": [
        "xs = range(-10,11)\n",
        "actuals =[devivative(x) for x in xs]\n",
        "estimates = [difference_quotient(square,x,h=0.001) for x in xs]\n",
        "\n",
        "#畫出圖形來比較兩者\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "plt.plot(xs, actuals ,'rx',label='Actual')    #用紅色x表示\n",
        "plt.plot(xs, estimates ,'b+',label='Estimates') #用藍色+表示\n",
        "plt.legend(loc=9)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7yRQ1Wrs9vM"
      },
      "source": [
        "## 偏導數\n",
        "如果f是個多變數函數，就會有多個偏導數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51prdTqtJMQ"
      },
      "source": [
        "def partitial_difference_quotient(f:Callable[[float],float],v:Vector,i:int,h:float) ->float:\n",
        "  \"\"\"送回f在v中第i個元素所對應的差商\"\"\"\n",
        "  w = [v_j + (h if i else 0) for i in range(len(v))]\n",
        "  return (f(w)-f(v))/ h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Xz1vif8q05"
      },
      "source": [
        "### 利用商差估計梯度\n",
        "缺點是對於計算能力要求很高，  \n",
        "若v有n個元素，則會進行2n個運算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqlncUB-8sSB"
      },
      "source": [
        "def estimate_gradient(f:Callable[[float],float],v:Vector,h:float = 0.0001):\n",
        "  return [partial_difference_quotient(f,v,i,h) for i in range(len(v))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY3LH8kF_Qj5"
      },
      "source": [
        "# 梯度的應用\n",
        "\n",
        "透過梯度遞減，在眾多三維向量中試著求出最小值。  \n",
        "1. 以隨機方式挑出一個起始點\n",
        "2. 沿著梯度的反方向移動一小步\n",
        "3. 重複動作1.2.直到梯度變得非常小為止"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ftOmpRQ__rB"
      },
      "source": [
        "import random\n",
        "\n",
        "def gradient_step(v:Vector,gradient:Vector,step_size:float) -> Vector:\n",
        "  \"\"\"從v沿著gradient的方向移動step_size的距離\"\"\"\n",
        "  assert len(v) == len(gradient)\n",
        "  step = scalar_multiply(step_size,gradient)\n",
        "  return add(v,step)\n",
        "\n",
        "def sum_of_squares_gradient(v:Vector) -> Vector:\n",
        "  return [2*v_i for v_i in v]\n",
        "\n",
        "#取個隨機起點\n",
        "v = [random.uniform(-10,10) for i in range(3)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  grad = sum_of_squares_gradient(v) # 計算出v所對應的梯度\n",
        "  v = gradient_step(v,grad,-0.01)  # 往梯度的負方向跨一小步\n",
        "  print(epoch,v)\n",
        "\n",
        "assert distance(v,[0,0,0]) < 0.001  # v應該會很接近0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1spR2a6DE2Y"
      },
      "source": [
        "隨著執行的階段越多，v就越靠近[0,0,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5A6qEmRDRFH"
      },
      "source": [
        "## 選擇正確的跨步間隔\n",
        "* 使用固定的間隔\n",
        "* 隨時間逐漸縮小間隔\n",
        "* 每個步驟都重新能夠讓目標函數數值最小話的間隔"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFs0UhCTDzMH"
      },
      "source": [
        "# 利用梯度遞減的做法來套入模型\n",
        "運用梯度遞減的做法把參數化模型套入到資料中，這些模型通常具有一個或多個參數。這些可微分的參數負責維繫著模型與資料間的關係。並搭配一個**損失函數(loss function)**來衡量模型與資料的匹配程度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-zT1aTZHpUJ"
      },
      "source": [
        "inputs = [(x,20*x+5) for x in range(-50,50)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZCieLjnw7NA"
      },
      "source": [
        "def linear_gradient(x:float,y:float,theta:Vector) ->Vector:\n",
        "  slope,intercept = theta\n",
        "  predicted = slope * x + intercept # 模型的預測值\n",
        "  error = (predicted - y)       # 誤差為(預測值-實際值)\n",
        "  squared_error = error**2       # 我們要設法最小化平方誤差\n",
        "  grad = [2*error*x,2*error]      # 過程中會用到梯度值\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHV2Vebf3YAI"
      },
      "source": [
        "針對整個資料集，查看軍方差的值。  \n",
        "而均方差的梯度值，其實就是各個梯度值的平均值。  \n",
        "  \n",
        "1. 首先從一組隨機的theta值開始\n",
        "2. 計算出梯度的隨機值\n",
        "3. 沿著梯度方向調整theta參數值\n",
        "4. 重複步驟2、3的動作。\n",
        "\n",
        "經過很多階段(epoch)之後，就能慢慢地學習得出正確的參數值："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4_N6yDP4uf3"
      },
      "source": [
        "#首先用一組隨機值，最為斜率與截距的起始值\n",
        "theta = [random.uniform(-1,1),random.uniform(-1,1)]\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(5000):\n",
        "  #計算梯度的平均值\n",
        "  grad = vector_mean([linear_gradient(x,y,theta) for x,y in inputs])\n",
        "  #延梯度方向移動一步\n",
        "  theta = gradient_step(theta,grad,-learning_rate)\n",
        "  print(epoch,theta)\n",
        "\n",
        "slope,intercept = theta\n",
        "\n",
        "print(\"斜率：\",slope)\n",
        "print(\"截距：\",intercept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU-s7N-6IFpF"
      },
      "source": [
        "# 小批量梯度遞減 (minibatch gradient descent)\n",
        "自較大的資料集取出小批量資料，再根據小批量資料計算相應的梯度值。(然後再沿著這個梯度方向更新參數值)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe_cgZZhIp_8"
      },
      "source": [
        "from typing import TypeVar , List , Iterator\n",
        "\n",
        "T = TypeVar('T') #用他建立「各種型別皆可使用的」函式\n",
        "\n",
        "def minibatches(dataset:List[T],\n",
        "         batch_size:int,\n",
        "         shuffle:bool=True)->Iterator[List[T]]:\n",
        "  \"\"\"自資料集中取出大小為batch_sixe的小批量資料\"\"\"\n",
        "  #起始索引值分別為0、batch_size、2*batch_size\n",
        "  batch_starts = [start for start in range(0,len(dataset),batch_size)]\n",
        "  if shuffle:random.shuffle(batch_starts) #打亂批量資料順序\n",
        "\n",
        "  for start in batch_starts:\n",
        "    end = start + batch_size\n",
        "    yield dataset[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYFlfSnWKjJ6"
      },
      "source": [
        "TypeVar(T)能被用來建立一個「各種型別皆可使用的」函式，輸入跟輸出都必須是相同的型別"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WznsVOSzKxoI"
      },
      "source": [
        "theta = [random.uniform(-1,1),random.uniform(-1,1)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for batch in minibatches(inputs,batch_size=20):\n",
        "    grad = vector_mean([linear_gradient(x,y,theta) for x,y in batch])\n",
        "    theta = gradient_step(theta,grad,-learning_rate)\n",
        "  print(epoch,theta)\n",
        "\n",
        "slope,intercept = theta\n",
        "\n",
        "print(\"斜率：\",slope)\n",
        "print(\"截距：\",intercept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywwepriMMPaZ"
      },
      "source": [
        "## 隨機梯度遞減(stochastic gradient descent)\n",
        "一次只使用一個訓練樣本，以計算相應的梯度並沿該方向更新參數值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwE33xb0M1NL"
      },
      "source": [
        "theta = [random.uniform(-1,1),random.uniform(-1,1)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for x,y in inputs:\n",
        "    grad = linear_gradient(x,y,theta)\n",
        "    theta = gradient_step(theta,grad,-learning_rate)\n",
        "  print(epoch,theta)\n",
        "\n",
        "slope,intercept = theta\n",
        "\n",
        "print(\"斜率：\",slope)\n",
        "print(\"截距：\",intercept)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}